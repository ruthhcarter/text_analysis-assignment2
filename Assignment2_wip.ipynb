{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c90c5f-9eb1-4896-bfc0-eda16ae3be41",
   "metadata": {},
   "source": [
    "# PSY 341K Text Analysis for Behavioral Data Science\n",
    "##### Spring 2024; written by: Prof Desmond Ong (desmond.ong@utexas.edu)\n",
    "\n",
    "## Assignment 2\n",
    "\n",
    "In this assignment we'll be processing a dataset using an NLP pipeline to extract linguistic features (n-grams), and then using these features to predict an outcome of interest.\n",
    "\n",
    "In Assignment 1, we walked you through each step of the 'research' process. In Assignment 2, we'll guide you through the high-level goals, but you'll have a bit more latitude to decide how to go about each step in the process. (You have all the \"mechanics\" in terms of the code required, from the Tutorials). This assignment will be more challenging because it is a bit more open-ended, and the idea is to gradually build you towards executing your research project, which is the other extreme where you decide everything (and it does get pretty overwhelming!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa65733-b0e4-4b69-af9c-a618f3916a31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/ruthcarter/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc23427f-3531-415b-9bc0-33bb5a354013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import twitter_samples\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda73b21-079a-4e69-bf55-c483c13eaf17",
   "metadata": {},
   "source": [
    "The data we are using is a small sample of tweets that is packaged into `nltk.corpus`, and they are labelled as either \"positive\" or \"negative\".\n",
    "\n",
    "\n",
    "### Your goal in this Assignment is to investigate what linguistic features predict a tweet being \"Positive\" or \"Negative\". \n",
    "\n",
    "In other words, you will calculate some linguistic features of interest, e.g., unigrams, bigrams, if you like, trigrams. You may also calculate other features, e.g., word count.\n",
    "\n",
    "You will then use those features in a logistic regression (or other classification technique of your choosing) to predict the label of the tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b4adb-e428-48d0-9794-a78e5867a269",
   "metadata": {},
   "source": [
    "### Reading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21373a7-fcc5-4a59-82f0-b1d1d48dbda6",
   "metadata": {},
   "source": [
    "The data consists of 5,000 positive and 5,000 negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a8ffa95-f26a-443f-ad1a-21e484ae82c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "print(len(positive_tweets), len(negative_tweets))\n",
    "\n",
    "all_tweets = positive_tweets + negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ea76a15-faf6-4412-81c2-1e44df4e3e28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)', '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!', '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!', '@97sides CONGRATS :)', 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days', '@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM', \"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\", '@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.', 'Jgh , but we have to go to Bayan :D bye', 'As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\\n\\nWell… as the name implies :p.']\n",
      "['hopeless for tmr :(', \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\", '@Hegelbon That heart sliding into the waste basket. :(', '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too', 'Dang starting next week I have \"work\" :(', \"oh god, my babies' faces :( https://t.co/9fcwGvaki0\", '@RileyMcDonough make me smile :((', '@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln', 'why?:(\"@tahuodyy: sialan:( https://t.co/Hv1i0xcrL2\"', 'Athabasca glacier was there in #1948 :-( #athabasca #glacier #jasper #jaspernationalpark #alberta #explorealberta #… http://t.co/dZZdqmf7Cz']\n"
     ]
    }
   ],
   "source": [
    "# print out the first ten positive tweets, and the first ten negative tweets, to get a sense of the text.\n",
    "\n",
    "# --- your code ---\n",
    "print(positive_tweets[:10])\n",
    "\n",
    "print(negative_tweets[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bdde0b-dba7-4589-a7b1-3673a740f2d1",
   "metadata": {},
   "source": [
    "#### Comment on your observations! What do you notice? Are there things that you have to take note of?\n",
    "\n",
    "#### Your Written Answer here\n",
    "- The positive tweets use a lot of smiley faces and exclamation marks. They also seem to interact with other users more often. There are also abbreviations of words (like tmr for tomorrow or fb for facebook, or ETL for... something...) that we need to consider. Otherwise, we won't fully understand the meaning of the tweet. Also, since hashtags are usually one whole word, it will be difficult to analyze (like #FlipkartFashionFriday). Also, not many tweets in this dataset seem to use emojis, but it looks like it does occasionally happen. I think this means we should use the \"from nltk.tokenize.casual import TweetTokenizer\" instead of normally tokenizing the text. Additionally, since tweets are popular in many countries, we may need to account for different spellings of English words (like neighbour and neighbor). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffb8a5f-97f7-4e70-9aaf-9ec30fdacc68",
   "metadata": {},
   "source": [
    "### Calculating Features\n",
    "\n",
    "Let's process the tweets! In class, we've covered a number of different preprocessing steps to calculate linguistic features. The choice of which steps to use (or not use) really depends on the specific context.\n",
    "\n",
    "- For example, we talked about why stop words are removed, but also why it may be interesting to keep stop words.\n",
    "    - \n",
    "- As another example, we talked about identifing Named Entities. But what do you do with them? You could decide to keep them in as features if you have specific hypotheses (e.g., if you're studying some political text, it might be handy to keep in the names of certain politicians). Or you might decide that actually names are irrelevant to your research question and remove them.\n",
    "    - might not be relevant, since we're just looking at whether tweets are positive or negative. there's not really any specific entities that would aid in whether tweets are pos or neg. \n",
    "\n",
    "The key is to really take some time to understand your data, and especially as it pertains to your hypotheses. As in Assignment 1, please `print()` and read some of the examples to get a sense for the language used. Please also `print()` out your variables as you are calculating them. Then you might notice additional issues that you may need to correct. \n",
    "\n",
    "- A simple one that we didn't cover in class (because it's quite straightforward) is lower-case normalization: that is, converting all the text to lowercase, say using `.lower()`. This is so `A strawberry` and `a strawberry` will become the same bigram. BUT lower-casing will also make `American` into `american`. (Also, lower-casing will throw off the POS-taggers/NER identifiers, which are case sensitive).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59d9970-4c38-4a75-976c-016e668db3e4",
   "metadata": {},
   "source": [
    "#### Note: Creating a feature array\n",
    "\n",
    "Note that after you preprocess and calculate your unigrams/n-grams, you need to convert the features into a large word-count array, with a corresponding \"vocabulary\". \n",
    "\n",
    "For example, we can take all the unigrams and arrange them alphabetically:\n",
    "\n",
    "- [\"American\", \"and\", ...]\n",
    "\n",
    "and if the first tweet is \"this American is a proud American\" (so 2 `American`s and 0 `and`s), and the second tweet is \"and I am happy\" (0 `American` and 1 `and`), then we need to create a feature array that looks like:\n",
    "\n",
    "- [[2, 0, ...]\n",
    "- [0, 1, ...]\n",
    "- ... ]\n",
    "\n",
    "such that the rows give the features for each tweet, while the columns give the word-count of each n-gram in the vocabulary. This `num_tweet` by `num_feature` array can then be used as the independent variables (\"X\") in the regression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95816249-8ef2-45f8-bc0a-9610cbaddb55",
   "metadata": {},
   "source": [
    "## Please process the text and calculate unigram and bigram features for each tweet.\n",
    "\n",
    "Please add as many code boxes as you need, and document your steps (e.g., with markdown chunks or with in-line comments).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7470a53-99d8-4c60-9a77-afa3bc2a35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3aecee3-aebe-4baa-976a-75cdc57d4377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['#followfriday', '@france_inte', '@pkuchly57', '@milipol_paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)'], ['@lamb2ja', 'hey', 'james', '!', 'how', 'odd', ':/', 'please', 'call', 'our', 'contact', 'centre', 'on', '02392441234', 'and', 'we', 'will', 'be', 'able', 'to', 'assist', 'you', ':)', 'many', 'thanks', '!'], ['@despiteofficial', 'we', 'had', 'a', 'listen', 'last', 'night', ':)', 'as', 'you', 'bleed', 'is', 'an', 'amazing', 'track', '.', 'when', 'are', 'you', 'in', 'scotland', '?', '!'], ['@97sides', 'congrats', ':)'], ['yeaaaah', 'yippppy', '!', '!', '!', 'my', 'accnt', 'verified', 'rqst', 'has', 'succeed', 'got', 'a', 'blue', 'tick', 'mark', 'on', 'my', 'fb', 'profile', ':)', 'in', '15', 'days']]\n",
      "[['hopeless', 'for', 'tmr', ':('], ['everything', 'in', 'the', 'kids', 'section', 'of', 'ikea', 'is', 'so', 'cute', '.', 'shame', \"i'm\", 'nearly', '19', 'in', '2', 'months', ':('], ['@hegelbon', 'that', 'heart', 'sliding', 'into', 'the', 'waste', 'basket', '.', ':('], ['“', '@ketchburning', ':', 'i', 'hate', 'japanese', 'call', 'him', '\"', 'bani', '\"', ':(', ':(', '”', 'me', 'too'], ['dang', 'starting', 'next', 'week', 'i', 'have', '\"', 'work', '\"', ':(']]\n"
     ]
    }
   ],
   "source": [
    "#tokenize the tweets: \n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "ttokenizer = TweetTokenizer() # here we have to create a TweetTokenizer object\n",
    "\n",
    "positive_tweets_list = [] \n",
    "negative_tweets_list = [] \n",
    "\n",
    "\n",
    "#tokenizing pos tweets\n",
    "for i in positive_tweets: \n",
    "    j= i.lower()\n",
    "    tokenized = ttokenizer.tokenize(j)\n",
    "    positive_tweets_list.append(tokenized)\n",
    "    \n",
    "print(positive_tweets_list[0:5])\n",
    "\n",
    "\n",
    "\n",
    "#tokenizing neg tweets\n",
    "for i in negative_tweets: \n",
    "    j= i.lower()\n",
    "    tokenized = ttokenizer.tokenize(j)\n",
    "    negative_tweets_list.append(tokenized)\n",
    "    \n",
    "print(negative_tweets_list[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65be9d99-ddcc-4979-be1d-bcf3eab288d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculating unigram features \n",
    "\n",
    "\n",
    "#unigram feat for pos tweets: \n",
    "unigram_pos_freq = []\n",
    "\n",
    "for i in positive_tweets_list: \n",
    "    unigram_frequency_distribution = nltk.FreqDist(i)\n",
    "    unigram_pos_freq.append(unigram_frequency_distribution)\n",
    "\n",
    "unigram_pos = [tuple(entry.items()) for entry in unigram_pos_freq]\n",
    "\n",
    "#unigram feat for neg tweets: \n",
    "unigram_neg_freq = []\n",
    "\n",
    "for i in negative_tweets_list: \n",
    "    unigram_frequency_distribution = nltk.FreqDist(i)\n",
    "    unigram_neg_freq.append(unigram_frequency_distribution)\n",
    "\n",
    "unigram_neg = [tuple(entry.items()) for entry in unigram_neg_freq]\n",
    "\n",
    "\n",
    "all_unigrams = unigram_pos + unigram_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59aeba2f-5e5e-405a-b4af-c132166301e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d448cf95-a26e-48a6-b37d-8801e17cef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating unigram feature array: \n",
    "\n",
    "flattened_list = [item for sublist in all_unigrams for item in sublist]\n",
    "\n",
    "unigrams_df = pd.DataFrame(flattened_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "643b52ef-19c3-4ab6-8375-5a53d502bb0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#followfriday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@france_inte</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@pkuchly57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@milipol_paris</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121042</th>\n",
       "      <td>expecting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121043</th>\n",
       "      <td>misserable</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121044</th>\n",
       "      <td>few</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121045</th>\n",
       "      <td>weeks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121046</th>\n",
       "      <td>:-(</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121047 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0  1\n",
       "0        #followfriday  1\n",
       "1         @france_inte  1\n",
       "2           @pkuchly57  1\n",
       "3       @milipol_paris  1\n",
       "4                  for  1\n",
       "...                ... ..\n",
       "121042       expecting  1\n",
       "121043      misserable  1\n",
       "121044             few  1\n",
       "121045           weeks  1\n",
       "121046             :-(  1\n",
       "\n",
       "[121047 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams_df[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a969c18f-7a49-49cb-9666-7ff80d27a52d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculating bigram features: \n",
    "\n",
    "#for pos tweets: \n",
    "bigram_pos_freq = [] \n",
    "\n",
    "for entry in positive_tweets_list:\n",
    "    bigram_pos_zip = []\n",
    "    text_for_pos_bigrams = nltk.ngrams(entry, 2)\n",
    "    bigram_pos_zip.append(text_for_pos_bigrams)\n",
    "    for zip_text in bigram_pos_zip: \n",
    "        bigram_frequency_distribution = nltk.FreqDist(zip_text)\n",
    "        bigram_pos_freq.append(bigram_frequency_distribution)\n",
    "bigram_pos = [tuple(entry.items()) for entry in bigram_pos_freq]\n",
    "\n",
    "\n",
    "#for neg tweets: \n",
    "bigram_neg_freq = [] \n",
    "\n",
    "for entry in negative_tweets_list:\n",
    "    bigram_neg_zip = []\n",
    "    text_for_neg_bigrams = nltk.ngrams(entry, 2)\n",
    "    bigram_neg_zip.append(text_for_neg_bigrams)\n",
    "    for zip_text in bigram_neg_zip: \n",
    "        bigram_frequency_distribution = nltk.FreqDist(zip_text)\n",
    "        bigram_neg_freq.append(bigram_frequency_distribution)\n",
    "bigram_neg = [tuple(entry.items()) for entry in bigram_neg_freq]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e58425-97ae-4af6-8508-24eb7e994176",
   "metadata": {},
   "source": [
    "\n",
    "### Creating a Train/Test set Split\n",
    "\n",
    "Now that we're done calculating features and are ready to move onto making predictions, let's split up the examples into a training set and a test set, in order to avoid overfitting.\n",
    "\n",
    "Please split up the data with **80% in the training set** and the remaining **20% in the test set**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac18350-c698-44a6-8fda-3b91d5c1430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can do a simple version (e.g., put the first 4000 \n",
    "# positive examples into a training set)\n",
    "# or you can also choose a random split.\n",
    "\n",
    "# be sure to create labels too!\n",
    "# Let's use 1 = pos, 0 = neg\n",
    "\n",
    "# --- your code ---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd8720-043b-4396-9739-5637a18657cc",
   "metadata": {},
   "source": [
    "### Using the linguistic features to make predictions\n",
    "\n",
    "Now that you have calculated the linguistic features for each tweet, and you have divided your data into a training set and a test set, let's take stock of the main variables you should have. (Note: *num_features* may differ depending on the choices you made to calculate your features.)\n",
    "\n",
    "You should have:\n",
    "\n",
    "- a 8000 x *num_features* array that contains the features for the 8000 tweets in the training set\n",
    "- a 2000 x *num_features* array that contains the features for the 2000 tweets in the test set\n",
    "- a 8000 x 1 array that contains the labels (pos/neg) for the 8000 tweets in the training set\n",
    "- a 2000 x 1 array that contains the labels (pos/neg) for the 2000 tweets in the test set\n",
    "\n",
    "- an array that contains information on what each of the features mean (i.e., for the unigram/bigram features, this refers to the \"vocabulary\". You'll need this to interpret the results.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff50951-a6f8-420a-adb7-d8c6a06e0c0d",
   "metadata": {},
   "source": [
    "If you've made it thus far, great, you're almost there! The remaining bit of work is to (i) train a model on the training set, (ii) evaluate the classification accuracy on the test set, and (iii) evaluate the features that are predictive of the label (and discuss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a13054e-e44f-4781-bb02-e734b5d23dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (i) Train a model on the training set\n",
    "\n",
    "# please set up and train a logistic regression model on the training set.\n",
    "# if your number of features is much larger than the training set size, you may wish to consider using regularization\n",
    "\n",
    "\n",
    "# NOTE if you are using statsmodel, this may take a while to train.\n",
    "# sklearn does it much faster. \n",
    "\n",
    "# --- your code ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3683bce8-cef3-467d-a25c-07ad42bc79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ii) Evaluate the classification accuracy on the test set\n",
    "\n",
    "# Using your model, make label predictions on the test set (by using the model on the test features).\n",
    "# compare them against the actual test set labels.\n",
    "# what is the classification accuracy of this model?\n",
    "\n",
    "# --- your code ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba52882-92ed-47e2-ad58-5fb01b01b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (iii) Evaluate the features that are predictive of the label (and discuss).\n",
    "\n",
    "# Take a look at the features that are most predictive of the label. \n",
    "# For example, which unigrams or bigrams were most predictive?\n",
    "# do these make sense?\n",
    "\n",
    "# --- your code, and written text response ---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
